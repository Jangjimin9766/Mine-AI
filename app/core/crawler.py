import requests
from bs4 import BeautifulSoup

def crawl_fashion_site(url: str) -> dict:
    """
    ì£¼ì–´ì§„ URLì—ì„œ ì œëª©, ë³¸ë¬¸ í…ìŠ¤íŠ¸, ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    print(f"ğŸ•¸ï¸ Starting crawl for: {url}")
    
    # 1. ë´‡ íƒì§€ë¥¼ í”¼í•˜ê¸° ìœ„í•œ í—¤ë” ì„¤ì • (ë§ˆì¹˜ ì›¹ë¸Œë¼ìš°ì €ì¸ ì²™í•˜ê¸°)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    try:
        # 2. HTML ìš”ì²­
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # 200 OKê°€ ì•„ë‹ˆë©´ ì—ëŸ¬ ë°œìƒ
        
        # 3. íŒŒì‹± (HTML ë¶„ì„)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # --- ë°ì´í„° ì¶”ì¶œ ë¡œì§ (ì‚¬ì´íŠ¸ë§ˆë‹¤ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ - ì¼ë°˜ì ì¸ êµ¬ì¡° ê¸°ì¤€) ---
        
        # [ì œëª©] h1 íƒœê·¸ ë˜ëŠ” title íƒœê·¸
        title_tag = soup.find('h1')
        if not title_tag:
            title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "No Title"

        # [ë³¸ë¬¸] p íƒœê·¸ë“¤ì„ ëª¨ì•„ì„œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
        paragraphs = soup.find_all('p')
        content = " ".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 10])
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ 500ìë§Œ ìë¥´ê¸° (í…ŒìŠ¤íŠ¸ìš©)
        content_snippet = content[:500] + "..." if len(content) > 500 else content

        # [ì´ë¯¸ì§€] og:image (SNS ê³µìœ ìš© ì¸ë„¤ì¼)ê°€ ê°€ì¥ í€„ë¦¬í‹°ê°€ ì¢‹ìŒ
        image_url = ""
        og_image = soup.find('meta', property='og:image')
        if og_image:
            image_url = og_image.get('content')
        
        print(f"âœ… Crawl success: {title}")

        return {
            "status": "success",
            "url": url,
            "title": title,
            "image_url": image_url,
            "content": content_snippet, # ìš”ì•½ ë° íƒœê¹…ì— ì‚¬ìš©í•  ë³¸ë¬¸
            "full_content": content     # ì „ì²´ ë³¸ë¬¸ (í•„ìš”ì‹œ ì‚¬ìš©)
        }
    
    except Exception as e:
        print(f"âŒ Error during crawling: {e}")
        return {
            "status": "fail",
            "url": url,
            "error": str(e)
        }