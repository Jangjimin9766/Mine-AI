from app.models.agent import ChatRequest
from app.core.llm_client import llm_client # [ë³€ê²½] ìš°ë¦¬ê°€ ë§Œë“  í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
import json

def get_llm_summary(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ìš”ì•½
    """
    if not text: return ""
    print(f"ğŸ§  AI Summarizing text (length: {len(text)})...")
    
    system_prompt = "You are a professional fashion editor. Summarize the following fashion article in Korean within 3 sentences. Focus on key trends and items."
    
    # [ë³€ê²½] ì§ì ‘ í˜¸ì¶œ -> llm_client ì‚¬ìš©
    return llm_client.generate_text(system_prompt, text, temperature=0.5)

def get_tags_from_text(text: str) -> list:
    """
    íƒœê·¸ ì¶”ì¶œ
    """
    if not text: return []
    print(f"ğŸ·ï¸ AI Tagging text...")

    system_prompt = """
    Analyze the following fashion text and extract relevant tags.
    Categories: Style (e.g., Minimal, Vintage), Mood (e.g., Chic, Cozy), Item (e.g., Coat, Boots).
    Output format: A simple Python list of strings in Korean. Example: ["ë¯¸ë‹ˆë©€", "ì‹œí¬", "ë¡±ì½”íŠ¸"]
    Do not write anything else, just the list.
    """
    
    # [ë³€ê²½] llm_client ì‚¬ìš©
    result_text = llm_client.generate_text(system_prompt, text, temperature=0.3)
    
    try:
        if "[" in result_text and "]" in result_text:
            return eval(result_text)
        else:
            return result_text.split(",")
    except:
        return []

def get_agent_response(request: ChatRequest, data: dict) -> str:
    """
    AI ì—ì´ì „íŠ¸ ì±„íŒ…
    """
    print(f"ğŸ’¬ AI Agent thinking for user {request.user_id}...")

    system_prompt = """
    You are 'M:ine', a professional and trendy personal fashion curator.
    
    [Your Role]
    - Analyze the provided data (search results or user collection) and the question.
    - Recommend styles or items that fit the context.
    - Speak in a friendly, stylish tone (Korean).
    - Use emojis occasionally to keep it lively.
    
    [Context Data]
    {data}
    """
    
    formatted_system = system_prompt.format(data=json.dumps(data, ensure_ascii=False))
    
    # [ë³€ê²½] llm_client ì‚¬ìš©
    return llm_client.generate_text(formatted_system, request.message, temperature=0.7)