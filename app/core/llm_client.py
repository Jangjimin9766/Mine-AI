from openai import OpenAI
from app.config import settings
import json
import traceback


# ========== Custom Exceptions ==========
class LLMClientError(Exception):
    """LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë ¨ ê¸°ë³¸ ì˜ˆì™¸"""
    pass

class APIKeyNotConfiguredError(LLMClientError):
    """API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ë•Œ ë°œìƒ"""
    pass

class LLMGenerationError(LLMClientError):
    """LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ì‹œ ë°œìƒ"""
    pass


class LLMClient:
    def __init__(self):
        # ì§€ì—° ì´ˆê¸°í™”ë¥¼ ìœ„í•´ Noneìœ¼ë¡œ ì‹œì‘
        self._client = None
        self.default_model = "gpt-3.5-turbo"

    def _get_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì§€ì—° ì´ˆê¸°í™”í•˜ì—¬ ë°˜í™˜"""
        if self._client is None:
            api_key = settings.OPENAI_API_KEY
            if not api_key or api_key == "test-key":
                return None  # í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œëŠ” None ë°˜í™˜
            self._client = OpenAI(api_key=api_key)
        return self._client
    
    def is_configured(self) -> bool:
        """API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        api_key = settings.OPENAI_API_KEY
        return api_key and api_key != "test-key"

    def generate_text(self, system_prompt: str, user_prompt: str, temperature: float = 0.7, raise_on_error: bool = False) -> str:
        """
        ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ìƒì„± (ìš”ì•½, ì±„íŒ…, íƒœê¹… ë“±)
        
        Args:
            raise_on_error: Trueì´ë©´ ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ë˜ì§. Falseì´ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜.
        """
        client = self._get_client()
        if client is None:
            error_msg = "OpenAI API key not configured. Check OPENAI_API_KEY in .env"
            print(f"âš ï¸ {error_msg}")
            if raise_on_error:
                raise APIKeyNotConfiguredError(error_msg)
            return ""
        
        try:
            response = client.chat.completions.create(
                model=self.default_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
            )
            result = response.choices[0].message.content.strip()
            if not result:
                print("âš ï¸ LLM returned empty response")
            return result
        except Exception as e:
            error_msg = f"LLM Text Generation Error: {e}"
            print(f"âŒ {error_msg}")
            print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
            if raise_on_error:
                raise LLMGenerationError(error_msg) from e
            return ""

    def generate_json(self, system_prompt: str, user_prompt: str, temperature: float = 0.7, raise_on_error: bool = False) -> dict:
        """
        JSON í¬ë§· ê°•ì œ ìƒì„± (ë§¤ê±°ì§„ ë°ì´í„° ìƒì„±ìš©)
        
        Args:
            raise_on_error: Trueì´ë©´ ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ë¥¼ ë˜ì§. Falseì´ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜.
        """
        client = self._get_client()
        if client is None:
            error_msg = "OpenAI API key not configured. Check OPENAI_API_KEY in .env"
            print(f"âš ï¸ {error_msg}")
            if raise_on_error:
                raise APIKeyNotConfiguredError(error_msg)
            return {}
        
        try:
            response = client.chat.completions.create(
                model=self.default_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            if not content:
                print("âš ï¸ LLM returned empty JSON response")
                if raise_on_error:
                    raise LLMGenerationError("LLM returned empty JSON response")
                return {}
            
            result = json.loads(content)
            return result
        except json.JSONDecodeError as e:
            error_msg = f"LLM JSON Parse Error: {e}"
            print(f"âŒ {error_msg}")
            print(f"ğŸ“‹ Raw content: {content[:500] if content else 'None'}")
            if raise_on_error:
                raise LLMGenerationError(error_msg) from e
            return {}
        except Exception as e:
            error_msg = f"LLM JSON Generation Error: {e}"
            print(f"âŒ {error_msg}")
            print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
            if raise_on_error:
                raise LLMGenerationError(error_msg) from e
            return {}


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì–´ë””ì„œë“  llm_clientë§Œ ì„í¬íŠ¸í•˜ë©´ ë¨)
llm_client = LLMClient()